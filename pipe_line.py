# -*- coding: utf-8 -*-
"""Pipe line

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DF1stvj8wzgCI9EfYpoQ3cG2-ZGv0Y5G
"""

# Lecture by Mr Akhona Njeje.
# Topic, Data Pipelines, 4 Sept.

# Intro.
# A data pipeline is a series of steps that move & transform data from various sources to a destination where it can be analysed.

# Basic structure of a pipeline
def extract():
  # code of extracting data from source.
  pass

def transform():
  # code to clean & transform data.
  pass

def load():
  # code to load to destination.
  pass

def run_pipeline():
  raw_data = extract()
  transformed_data = transform_data(raW_data)
  load_data(transformed_data)

run_pipeline()

# Extraction : Reading from files.

import pandas as pd

def extract_data_from_csv(file_path):
  return pd.read_csv(file_path)

def extract_data_from_excel(file_path):
  return pd.read_excel(file_path)

# Example how to use it.

csv_data = extract_data_from_csv("data.csv")
excel_data = extract_data_from_excel("data.xlsx")

print(f"CSV  data shape: {csv_data.shape)}")
print(f"Extracted data shape{excel_data.shape}")

# API Requests.
# Lets fetch data using an API, basically we are extracting from various sources.

import requests
def extract_data_from_api(api_url):
  response = requests.get(api_url)
  if response.status_code == 200:
    return response.json()
  else:
    raise Exception(f"API request Failed:
    {response.status_code}")

# How youll use it.

api_url = "https://api.example.com/data"
raw_data = extract_data_from_api(api_url)
print(f"Extracted {len(raW_data)} records from API")

# Transformation P1 : Cleaning & Preprocessing.
# This step involves handling missing values, removing duplicates & transforming data types.

def clean_data(df):
  # Remove duplicates.
  df = df.drop_duplicates()
  # Handle missing values.
  df = df.fillna(df.mean(numeric_only=True))
  # Convert date columns values.
  date_columns = ["date_column1", "date_column2"]
  for col in date_columns:
    df[col] = pd.to_datetime(df[col])
  return df

# Example how to use it.

raw_data = pd.read_csv("raw_data.csv")
cleaned_data = clean_data(raw_data)
print(f"Transformed data shape: {cleaned_data.shape}")

# Transformation P2 : Feature Engineering.

def engineering_ft(df):
  # Create new features.
  df["total_amount"] = df["quantinty"]*df["price"]
  # Extract components from date time.
  df["year"] = df["date"].dt.year
  df["month"] = df["date"].dt.month
  df["day_of_week"] = df["date"].dt.dayofweek

  # Bin a continous variable.
  df["age_group"] = pd.cut(df["age"], bins=[0, 18, 30, 50, 100],
                           labels=["0-18", "18-30", "30-50", "51+"])
  return df

# Explore usage.

data = pd.read_csv("sales_data.csv")
data["date"] = pd.to_datetime(data["date"])
engineered_data = engineering_ft(data)
print(engineered_data.head())

# Loading P1 : Saving to csv.

def save_data_to_csv(df, file_path):
  df.to_csv(file_path, index=False)
  print(f"Data saved to {file_path}")

# Example usage.
transformed_data = pd.DataFrame({"A": [1, 2, 3], "B": ['x', 'y', 'z']})
save_data_to_csv(transformed_data, "output_data.csv")

# Loading P2 : Writing to a DB.
# After transforming our data we need to load to a destination where it be ready for Analysis or futher processing.

from sqlalchemy import create_engine

def load_to_database(df, table_name, connection_string):
  engine = create_engine(connection_string)
  df.to_sql(table_name, engine, if_exists="replace", index=False)
  print(f"Data loaded to {table_name}")

# Example usage.
connection_string = "sqlite:///my_database.db"
data_to_load = pd.DataFrame({"id": [1, 2, 3], "value": ['x', 'y', 'z']})
load_to_database(data_to_load, "my_table", connection_string)

# Parrallel processing with multi processing.
# We use these to improve perfomance in our data pipeline.

import multiprocessing

def process_data(data):
  # Code to process data.
  return data.apply(lambda x: x*2)

def parallel_processing(df, num_processes=4):
  pool = multiprocessing.Pool(processes=num_processes)
  data = np.array_split(df, num_processes)
  result = pool.map(process_data, data)
  return pd.concat(result)

# Example usage.

data1 = pd.DataFrame({"A": range(1000000)})
processed_data = parallel_processing(data1)
print(processed_data)

# Error handling & lodging.
# Error handling & logging helps to diagnoise & fix issues.

import logging
from functools import wraps

logging.basicConfig(level=logging.INFO, format='%(asctime)s-%(levelname)s-%(messages)')

def error(function):
  @wraps(function)
  def wrapper(*args, **kwargs):
    try:
      return function(*args, **kwargs)
    except Exception aas e:
      logging.error(f"Error in {function.__name__}: {str(e)}")
      raise
  return wrapper

@error_handler
def risky_operation(x):
  if x == 0:
    raise ValueError("Cannot divide by zero")
  return 10/x

# Example usage
for i in range(-1,2):
  try:
    result = risky_operation(i)
    logging.info(f"Result:{result}")
  except Exception:
    logging.info("Moving to next iteration")

# Data validation with great expectation.
# Ensuring data quality is crucial in data pipelines.
# Great Excpectations is a powerful library for validating, documenting & profiling data.

import great_expectations as ge
def validate_data(df):
  ge_df = ge.from_pandas(df)

  # define expectations
  ge_df.expect_column_values_to_between("age", min_value=0, max_value+120)
  ge_df.expect_column_values_to_not_be_null("name")
  e_df.expect_column_values_to_be_in_set("gender", ["M","F","OTHER"])
  # RUN VALIDATION.
  results = ge_df.validate()
  return results

# Example usage

data2 = pd.DataFrame({
    "name":['Ak', 'Akon', 'Akhona'],
    "age":[17,34,51],
    "gender":['F','M','Unkown']
})
validation_results = validate_data(data)
print(f"Validation sucessful:{validation_results.success}")
print(f"Number of exectations:{len(validation_results.results}")

